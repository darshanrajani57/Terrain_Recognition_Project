{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "895d2b34",
   "metadata": {},
   "source": [
    "# Deep Learning For Terrain Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0035ee22",
   "metadata": {},
   "source": [
    "## Step-1 Importing Libraries, Data PreProcessing and Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14a843f",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92a0cc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, MaxPool2D, Conv2D, Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb05a234",
   "metadata": {},
   "source": [
    "### Data PreProcessing and Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2fd130a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 31571 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1.0 / 255,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip = True,\n",
    "    validation_split = 0.2\n",
    ")\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    './Data Main/train',\n",
    "    target_size = (64, 64),  # Adjust the target image size\n",
    "    batch_size = 32,\n",
    "    class_mode = 'categorical',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f8123e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6769 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_set = test_datagen.flow_from_directory('./Data Main/test',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f7c54e",
   "metadata": {},
   "source": [
    "## Step-2 Building the CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99164010",
   "metadata": {},
   "source": [
    "### Instantiating The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5611a54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bf6199",
   "metadata": {},
   "source": [
    "### First Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da356dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(filters = 32, kernel_size = 3, activation = 'relu', input_shape = [64, 64, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b464649",
   "metadata": {},
   "source": [
    "### First Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac0ef56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(MaxPool2D(pool_size = 2, strides = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dc4951",
   "metadata": {},
   "source": [
    "### Second Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64057dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(filters = 32, kernel_size = 3, activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f38051",
   "metadata": {},
   "source": [
    "### Second Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba54de4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(MaxPool2D(pool_size = 2, strides = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec9fdd5",
   "metadata": {},
   "source": [
    "### Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41a9ca84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a67819d",
   "metadata": {},
   "source": [
    "### Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "68af5699",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units = 128, activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e236ed",
   "metadata": {},
   "source": [
    "### Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9f6071a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units = 4, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc01cf30",
   "metadata": {},
   "source": [
    "## Step-3 Compiling and Training The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccfcdc8",
   "metadata": {},
   "source": [
    "### Compiling The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9ab56dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = tf.keras.optimizers.legacy.Adam(), loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2020769",
   "metadata": {},
   "source": [
    "## Training The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "24d11b6d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "987/987 [==============================] - 143s 145ms/step - loss: 0.0559 - accuracy: 0.9820 - val_loss: 0.1610 - val_accuracy: 0.9521\n",
      "Epoch 2/25\n",
      "987/987 [==============================] - 147s 149ms/step - loss: 0.0562 - accuracy: 0.9817 - val_loss: 0.1011 - val_accuracy: 0.9676\n",
      "Epoch 3/25\n",
      "987/987 [==============================] - 145s 147ms/step - loss: 0.0545 - accuracy: 0.9817 - val_loss: 0.1280 - val_accuracy: 0.9583\n",
      "Epoch 4/25\n",
      "987/987 [==============================] - 147s 149ms/step - loss: 0.0545 - accuracy: 0.9822 - val_loss: 0.1482 - val_accuracy: 0.9580\n",
      "Epoch 5/25\n",
      "987/987 [==============================] - 146s 148ms/step - loss: 0.0491 - accuracy: 0.9833 - val_loss: 0.1385 - val_accuracy: 0.9577\n",
      "Epoch 6/25\n",
      "987/987 [==============================] - 148s 150ms/step - loss: 0.0474 - accuracy: 0.9846 - val_loss: 0.2057 - val_accuracy: 0.9394\n",
      "Epoch 7/25\n",
      "987/987 [==============================] - 147s 149ms/step - loss: 0.0549 - accuracy: 0.9824 - val_loss: 0.1521 - val_accuracy: 0.9575\n",
      "Epoch 8/25\n",
      "987/987 [==============================] - 146s 148ms/step - loss: 0.0443 - accuracy: 0.9851 - val_loss: 0.2285 - val_accuracy: 0.9453\n",
      "Epoch 9/25\n",
      "987/987 [==============================] - 145s 147ms/step - loss: 0.0468 - accuracy: 0.9846 - val_loss: 0.1803 - val_accuracy: 0.9465\n",
      "Epoch 10/25\n",
      "987/987 [==============================] - 152s 154ms/step - loss: 0.0469 - accuracy: 0.9850 - val_loss: 0.1996 - val_accuracy: 0.9539\n",
      "Epoch 11/25\n",
      "987/987 [==============================] - 161s 163ms/step - loss: 0.0529 - accuracy: 0.9828 - val_loss: 0.1177 - val_accuracy: 0.9657\n",
      "Epoch 12/25\n",
      "987/987 [==============================] - 150s 152ms/step - loss: 0.0421 - accuracy: 0.9854 - val_loss: 0.1287 - val_accuracy: 0.9640\n",
      "Epoch 13/25\n",
      "987/987 [==============================] - 154s 156ms/step - loss: 0.0507 - accuracy: 0.9833 - val_loss: 0.1602 - val_accuracy: 0.9536\n",
      "Epoch 14/25\n",
      "987/987 [==============================] - 147s 149ms/step - loss: 0.0435 - accuracy: 0.9859 - val_loss: 0.1788 - val_accuracy: 0.9560\n",
      "Epoch 15/25\n",
      "987/987 [==============================] - 137s 139ms/step - loss: 0.0368 - accuracy: 0.9880 - val_loss: 0.1459 - val_accuracy: 0.9651\n",
      "Epoch 16/25\n",
      "987/987 [==============================] - 127s 129ms/step - loss: 0.0442 - accuracy: 0.9853 - val_loss: 0.1426 - val_accuracy: 0.9582\n",
      "Epoch 17/25\n",
      "987/987 [==============================] - 128s 130ms/step - loss: 0.0402 - accuracy: 0.9870 - val_loss: 0.1374 - val_accuracy: 0.9644\n",
      "Epoch 18/25\n",
      "987/987 [==============================] - 129s 131ms/step - loss: 0.0385 - accuracy: 0.9874 - val_loss: 0.1116 - val_accuracy: 0.9705\n",
      "Epoch 19/25\n",
      "987/987 [==============================] - 128s 129ms/step - loss: 0.0414 - accuracy: 0.9861 - val_loss: 0.1341 - val_accuracy: 0.9616\n",
      "Epoch 20/25\n",
      "987/987 [==============================] - 127s 128ms/step - loss: 0.0394 - accuracy: 0.9877 - val_loss: 0.1588 - val_accuracy: 0.9586\n",
      "Epoch 21/25\n",
      "987/987 [==============================] - 127s 129ms/step - loss: 0.0408 - accuracy: 0.9866 - val_loss: 0.1226 - val_accuracy: 0.9651\n",
      "Epoch 22/25\n",
      "987/987 [==============================] - 131s 132ms/step - loss: 0.0381 - accuracy: 0.9874 - val_loss: 0.1455 - val_accuracy: 0.9592\n",
      "Epoch 23/25\n",
      "987/987 [==============================] - 132s 134ms/step - loss: 0.0413 - accuracy: 0.9871 - val_loss: 0.1403 - val_accuracy: 0.9620\n",
      "Epoch 24/25\n",
      "987/987 [==============================] - 130s 132ms/step - loss: 0.0437 - accuracy: 0.9865 - val_loss: 0.1647 - val_accuracy: 0.9557\n",
      "Epoch 25/25\n",
      "987/987 [==============================] - 131s 133ms/step - loss: 0.0328 - accuracy: 0.9893 - val_loss: 0.1599 - val_accuracy: 0.9577\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x28eed6490>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x = training_set, validation_data = test_set, epochs = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7640f0a",
   "metadata": {},
   "source": [
    "## Step-4 Making a Single Prediction (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7109be74",
   "metadata": {},
   "source": [
    "### Importing the Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4b568a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "image_path = './Data Main/val/Marshy/Marshy (1)_0_8.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbc08b2",
   "metadata": {},
   "source": [
    "### PreProcessing the Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "28a9161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load_img(image_path, target_size=(64, 64))\n",
    "\n",
    "img_array = img_to_array(img)\n",
    "img_array = img_array / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f709be",
   "metadata": {},
   "source": [
    "### Flattening the image array for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a5626922",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array = np.expand_dims(img_array, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80643128",
   "metadata": {},
   "source": [
    "### Predicting result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fc1b357c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 42ms/step\n",
      "Marshy\n"
     ]
    }
   ],
   "source": [
    "prediction_number = np.argmax(model.predict(img_array))\n",
    "\n",
    "class_mapping = {\n",
    "    0: 'Grassy',\n",
    "    1: 'Marshy',\n",
    "    2: 'Rocky',\n",
    "    3: 'Sandy'\n",
    "}\n",
    "\n",
    "predicted_class = class_mapping[prediction_number]\n",
    "\n",
    "print(predicted_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca2833e",
   "metadata": {},
   "source": [
    "## Step-5 Saving The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe4030e",
   "metadata": {},
   "source": [
    "### Saving the model with checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "33ab5e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('TerrainRecognitionModel.h5', save_format='h5')\n",
    "model.save('TerrainRecognitionModel.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a409c048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212/212 [==============================] - 13s 59ms/step - loss: 0.1599 - accuracy: 0.9577\n",
      "Test Loss: 0.15991055965423584\n",
      "Test Accuracy: 0.957748532295227\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "evaluation = model.evaluate(test_set)\n",
    "\n",
    "# The evaluation result is a list containing the loss and accuracy\n",
    "loss = evaluation[0]\n",
    "accuracy = evaluation[1]\n",
    "\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d88e664",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
